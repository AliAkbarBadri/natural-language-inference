{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of tempo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakbarbadri/natural-language-inference/blob/master/snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd0c727f-dfcf-4a3c-ed82-9cdaf6e9abf5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU, TimeDistributed\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7rnAUaISbK8",
        "colab_type": "text"
      },
      "source": [
        "https://www.tensorflow.org/datasets/catalog/snli\n",
        "\n",
        "https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu\n",
        "\n",
        "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSNRMGfComKc",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWeh9G4iRlCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = tfds.load('snli')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWwX25STJVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = ds['train']\n",
        "valid_ds = ds['validation']\n",
        "test_ds = ds['test']\n",
        "#labels: {0: 'entailment', 1: 'neutral', 2: 'contradiction' , -1: '-'}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBXfaIKjwAmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a7d458a4-c667-4811-e07f-690924c2c3c9"
      },
      "source": [
        "for example in train_ds.take(1):\n",
        "  print(example)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Washing clothes on a camping trip.'>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'A man washes or dies clothes in a primitive setting.'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amXfHj7vs4uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(w):\n",
        "  def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRLNhkglwtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af108c43-2a6c-4ebc-f66d-0d349c413ad5"
      },
      "source": [
        "# train_data = []\n",
        "# train_targets = []\n",
        "# valid_data = []\n",
        "# valid_targets = []\n",
        "# test_data = []\n",
        "# test_targets = []\n",
        "\n",
        "# for row in train_ds:\n",
        "#   if int(row[\"label\"]) == 0:\n",
        "#     train_data.append(preprocess(str(row[\"hypothesis\"].numpy())))\n",
        "#     train_targets.append(preprocess(str(row[\"premise\"].numpy())))\n",
        "  \n",
        "\n",
        "# for row in valid_ds:\n",
        "#   if int(row[\"label\"]) == 0:\n",
        "#     valid_data.append(preprocess(str(row[\"hypothesis\"].numpy())))\n",
        "#     valid_targets.append(preprocess(str(row[\"premise\"].numpy())))\n",
        "\n",
        "for row in test_ds.take(1):\n",
        "  if int(row[\"label\"]) == 0:\n",
        "    print(preprocess(row[\"hypothesis\"].numpy().decode(\"utf-8\")))\n",
        "\n",
        "    print(preprocess(row[\"premise\"].numpy().decode(\"utf-8\")))\n",
        "\n",
        "# print(\"train length\", len(train_data))\n",
        "# print(\"valid length\", len(valid_data))\n",
        "# print(\"test length\", len(test_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> a girl is entertaining on stage <end>\n",
            "<start> a girl in a blue leotard hula hoops on a stage with balloon shapes in the background . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbglu6bwqwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(num_examples):\n",
        "  pairs = [[preprocess(row[\"hypothesis\"].numpy().decode(\"utf-8\")), preprocess(row[\"premise\"].numpy().decode(\"utf-8\"))]  for row in train_ds.take(num_examples) if int(row[\"label\"])==0]\n",
        "  return zip(*pairs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1JbG-dL1PMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(num_examples=100000):\n",
        "  input_data, target_data = create_dataset(num_examples)\n",
        "  \n",
        "  # input_train, input_val, target_train, target_val = train_test_split(input_data, target_data, test_size=0.2)\n",
        "\n",
        "  \n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token = \"<OOV>\")\n",
        "  tokenizer.fit_on_texts(list(set().union(input_data,target_data)))\n",
        "  \n",
        "  input_tensor_temp = tokenizer.texts_to_sequences(input_data)\n",
        "  target_tensor_temp = tokenizer.texts_to_sequences(target_data)\n",
        "\n",
        "  input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor_temp,padding='post')\n",
        "  target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_temp,padding='post')\n",
        "\n",
        "  return input_tensor, target_tensor, tokenizer"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkLLBcKO5xLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85d924ab-4154-4981-d7e8-3a6f04ce1196"
      },
      "source": [
        "num_examples = 1000\n",
        "input_tensor, target_tensor, tokenizer = load_dataset(num_examples)\n",
        "max_length_input, max_length_target  = input_tensor.shape[1], target_tensor.shape[1]\n",
        "max_length_input, max_length_target"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1yLSGRTN9WB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "50aea2ee-9888-485b-d3b1-b02e40c6df92"
      },
      "source": [
        "tf.one_hot(target_tensor[0][1:],vocab_inp_size)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(44, 1228), dtype=float32, numpy=\n",
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjP0BZ9LNxfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "480a6cd3-87ea-4838-ea74-45c703317e3b"
      },
      "source": [
        "target_tensor[0][1:]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   32,   13,  283,   12, 1017,   11,  585,  169,    7,    2,\n",
              "        268,    7,    6,  286,   13,    2,   43,    5,    4,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6AtT0_qPdoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tensor_output = tf.one_hot(target_tensor[:,1:],vocab_inp_size)\n",
        "# target_tensor_output = target_tensor[:,1:]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3m9U_H_544G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2db9eb35-2901-43e3-b2a6-0e5ba8084775"
      },
      "source": [
        "len(input_tensor), len(target_tensor), len(target_tensor_output)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(362, 362, 362)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu2G4h9M6QQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(tokenizer, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, tokenizer.index_word[t]))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3RcHSF16SkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "dba76975-bda1-4168-99b1-7999952e4a4d"
      },
      "source": [
        "print (\"Input; index to word mapping\")\n",
        "convert(tokenizer, input_tensor[0])\n",
        "print ()\n",
        "print (\"Target; index to word mapping\")\n",
        "convert(tokenizer, target_tensor[0])"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input; index to word mapping\n",
            "3 ----> <start>\n",
            "29 ----> there\n",
            "11 ----> are\n",
            "283 ----> motorcycles\n",
            "7 ----> in\n",
            "2 ----> a\n",
            "268 ----> lot\n",
            "5 ----> .\n",
            "4 ----> <end>\n",
            "\n",
            "Target; index to word mapping\n",
            "3 ----> <start>\n",
            "2 ----> a\n",
            "32 ----> group\n",
            "13 ----> of\n",
            "283 ----> motorcycles\n",
            "12 ----> and\n",
            "1017 ----> scooters\n",
            "11 ----> are\n",
            "585 ----> parked\n",
            "169 ----> together\n",
            "7 ----> in\n",
            "2 ----> a\n",
            "268 ----> lot\n",
            "7 ----> in\n",
            "6 ----> the\n",
            "286 ----> middle\n",
            "13 ----> of\n",
            "2 ----> a\n",
            "43 ----> street\n",
            "5 ----> .\n",
            "4 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJvBdbsgfSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtBhrnIT7hCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(tokenizer.word_index)+1\n",
        "vocab_size = vocab_inp_size\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K6QtLFEJ-jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for e in dataset.take(1):\n",
        "#   print(len(e[0][0]))\n",
        "#   print(len(e[1][0]))"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbcofnX6ub5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9f645505-15e7-4a9f-8362-b0d707ae9be1"
      },
      "source": [
        "input_tensor[0]"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  3,  29,  11, 283,   7,   2, 268,   5,   4,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKm3EjF_7lCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example_input_batch, example_target_batch = next(iter(dataset))\n",
        "# example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0BAk5tl8cjq",
        "colab_type": "text"
      },
      "source": [
        "# tf example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhk2LWI8epI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hccS8inw8n2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2541c2f6-ba2f-41bf-d26b-795e58e30389"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 38, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZHXVwVi8tPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   initial_state=enc_output)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    # self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    # x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5mnIaHr82B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9623b626-0717-4851-b3cd-573c43605c04"
      },
      "source": [
        "decoder = Decoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_output)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 10653)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZTXPkuE8798",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtxiWSkq9cwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs4-U-Za9h_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "5aff6423-5e4e-4de9-d26c-e93a7cbc0453"
      },
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  # if (epoch + 1) % 2 == 0:\n",
        "  #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.6436\n",
            "Epoch 1 Batch 100 Loss 0.6475\n",
            "Epoch 1 Batch 200 Loss 0.6516\n",
            "Epoch 1 Batch 300 Loss 0.6314\n",
            "Epoch 1 Batch 400 Loss 0.6504\n",
            "Epoch 1 Loss 0.6445\n",
            "Time taken for 1 epoch 305.2927334308624 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5900\n",
            "Epoch 2 Batch 100 Loss 0.6335\n",
            "Epoch 2 Batch 200 Loss 0.6726\n",
            "Epoch 2 Batch 300 Loss 0.7075\n",
            "Epoch 2 Batch 400 Loss 0.6723\n",
            "Epoch 2 Loss 0.6335\n",
            "Time taken for 1 epoch 305.09351348876953 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6316\n",
            "Epoch 3 Batch 100 Loss 0.5713\n",
            "Epoch 3 Batch 200 Loss 0.6167\n",
            "Epoch 3 Batch 300 Loss 0.6290\n",
            "Epoch 3 Batch 400 Loss 0.5996\n",
            "Epoch 3 Loss 0.6258\n",
            "Time taken for 1 epoch 305.04081320762634 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0-dSwND50n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_input, max_length_target))\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_input,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_target):\n",
        "    predictions, dec_hidden = decoder(dec_input,dec_hidden,enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += tokenizer.index_word[predicted_id] + ' '\n",
        "    if tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRnfyxvAD65n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72PfhyIoMFnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b1aac9db-b6b1-4c28-8730-c4069c9715bd"
      },
      "source": [
        "translate(\"a girl is entertaining on stage\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> a girl is entertaining on stage <end>\n",
            "Predicted: a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}