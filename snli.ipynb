{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of tempo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakbarbadri/natural-language-inference/blob/master/snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import imageio"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSNRMGfComKc",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7RsBuPdxU83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODE = 'train'\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_SIZE = 256\n",
        "RNN_SIZE = 512\n",
        "NUM_EPOCHS = 15\n",
        "ATTENTION_FUNC = 'concat'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWeh9G4iRlCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/premises_train.pickle?raw=true')\n",
        "target = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/hypotheses_train.pickle?raw=true')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWwX25STJVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f06f5cc-9839-439f-ca87-b0a3ce6aea51"
      },
      "source": [
        "print(source[0])\n",
        "print(target[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A person on a horse jumps over a broken down airplane .\n",
            "A person is outdoors , on a horse .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLtsoYAx8b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = source[:5000]\n",
        "trg = target[:5000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBXfaIKjwAmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCZns_O0xiXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data_src = [normalize_string(data) for data in src]\n",
        "raw_data_trg_in = ['<start> ' + normalize_string(data) for data in trg]\n",
        "raw_data_trg_out = [normalize_string(data) + ' <end>' for data in trg]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMyMHDln3Lkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "430fb4ec-6aed-48d0-8dea-20c9839e43e5"
      },
      "source": [
        "src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "src_tokenizer.fit_on_texts(raw_data_src)\n",
        "data_src = src_tokenizer.texts_to_sequences(raw_data_src)\n",
        "data_src = tf.keras.preprocessing.sequence.pad_sequences(data_src,\n",
        "                                                        padding='post')\n",
        "print(data_src[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   1   52    6    1  229  209   70    1 1316   40  545    2    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]\n",
            " [  57  134    5  825   15   66    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1No4Dlqy1T--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "51eefa9b-ebc8-4d90-bd34-27b8c1c15947"
      },
      "source": [
        "trg_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "trg_tokenizer.fit_on_texts(raw_data_trg_in)\n",
        "trg_tokenizer.fit_on_texts(raw_data_trg_out)\n",
        "data_trg_in = trg_tokenizer.texts_to_sequences(raw_data_trg_in)\n",
        "data_trg_in = tf.keras.preprocessing.sequence.pad_sequences(data_trg_in,\n",
        "                                                           padding='post')\n",
        "print(data_trg_in[:2])\n",
        "\n",
        "data_trg_out = trg_tokenizer.texts_to_sequences(raw_data_trg_out)\n",
        "data_trg_out = tf.keras.preprocessing.sequence.pad_sequences(data_trg_out,\n",
        "                                                            padding='post')\n",
        "print(data_trg_out[:2])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  3   1  19   6  39  11   1 148   2   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  3  15   7  49 378   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "[[  1  19   6  39  11   1 148   2   4   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 15   7  49 378   4   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O20z0Pk3Sdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_src, data_trg_in, data_trg_out))\n",
        "dataset = dataset.shuffle(len(raw_data_src)).batch(\n",
        "    BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRLNhkglwtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, sequence, states):\n",
        "        embed = self.embedding(sequence)\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        return (tf.zeros([batch_size, self.lstm_size]),\n",
        "                tf.zeros([batch_size, self.lstm_size]))\n",
        "\n",
        "\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "\n",
        "encoder = Encoder(src_vocab_size, EMBEDDING_SIZE, RNN_SIZE)\n",
        "initial_state = encoder.init_states(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orP-CXN233Fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d311e1a0-14ed-412b-8747-f8cebc43338b"
      },
      "source": [
        "test_encoder_output = encoder(tf.constant(\n",
        "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
        "print(test_encoder_output[0].shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 6, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amFMhvO0-fZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "  def __init__(self, rnn_size, attention_func):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.attention_func = attention_func\n",
        "    if attention_func not in ['dot', 'general', 'concat']:\n",
        "      raise ValueError(\n",
        "        'Unknown attention score function! Must be either dot, general or concat.')\n",
        "    if attention_func == 'general':\n",
        "    # General score function\n",
        "      self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "    elif attention_func == 'concat':\n",
        "      # Concat score function\n",
        "      self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "      self.va = tf.keras.layers.Dense(1)\n",
        "  def call(self, decoder_output, encoder_output):\n",
        "    if self.attention_func == 'dot':\n",
        "      # Dot score function: decoder_output (dot) encoder_output\n",
        "      # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "      # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "      # => score has shape: (batch_size, 1, max_len)\n",
        "      score = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
        "    elif self.attention_func == 'general':\n",
        "      # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
        "      # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "      # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "      # => score has shape: (batch_size, 1, max_len)\n",
        "      score = tf.matmul(decoder_output, self.wa(encoder_output), transpose_b=True)\n",
        "    elif self.attention_func == 'concat':\n",
        "      # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
        "      # Decoder output must be broadcasted to encoder output's shape first\n",
        "      decoder_output = tf.tile(\n",
        "      decoder_output, [1, encoder_output.shape[1], 1])\n",
        "      # Concat => Wa => va\n",
        "      # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
        "      score = self.va(\n",
        "    self.wa(tf.concat((decoder_output, encoder_output), axis=-1)))\n",
        "    # Transpose score vector to have the same shape as other two above\n",
        "    # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
        "    score = tf.transpose(score, [0, 2, 1])\n",
        "    # alignment a_t = softmax(score)\n",
        "    alignment = tf.nn.softmax(score, axis=2)\n",
        "    # context vector c_t is the weighted average sum of encoder output\n",
        "    context = tf.matmul(alignment, encoder_output)\n",
        "    return context, alignment"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbglu6bwqwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, rnn_size, attention_func):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.attention = LuongAttention(rnn_size, attention_func)\n",
        "    self.rnn_size = rnn_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(\n",
        "    rnn_size, return_sequences=True, return_state=True)\n",
        "    self.wc = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "    self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self, sequence, state, encoder_output):\n",
        "    # Remember that the input to the decoder\n",
        "    # is now a batch of one-word sequences,\n",
        "    # which means that its shape is (batch_size, 1)\n",
        "    embed = self.embedding(sequence)\n",
        "    # Therefore, the lstm_out has shape (batch_size, 1, rnn_size)\n",
        "    lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
        "    # Use self.attention to compute the context and alignment vectors\n",
        "    # context vector's shape: (batch_size, 1, rnn_size)\n",
        "    # alignment vector's shape: (batch_size, 1, source_length)\n",
        "    context, alignment = self.attention(lstm_out, encoder_output)\n",
        "    # Combine the context vector and the LSTM output\n",
        "    # Before combined, both have shape of (batch_size, 1, rnn_size),\n",
        "    # so let's squeeze the axis 1 first\n",
        "    # After combined, it will have shape of (batch_size, 2 * rnn_size)\n",
        "    lstm_out = tf.concat(\n",
        "                [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "    # lstm_out now has shape (batch_size, rnn_size)\n",
        "    lstm_out = self.wc(lstm_out)\n",
        "    # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
        "    logits = self.ws(lstm_out)\n",
        "    return logits, state_h, state_c, alignment\n",
        "\n",
        "\n",
        "# trg_vocab_size = len(trg_tokenizer.word_index) + 1\n",
        "# decoder = Decoder(trg_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
        "# de_initial_state = test_encoder_output[1:]\n",
        "\n",
        "trg_vocab_size = len(trg_tokenizer.word_index) + 1\n",
        "decoder = Decoder(trg_vocab_size, EMBEDDING_SIZE, RNN_SIZE, ATTENTION_FUNC)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xovyWE9i4NJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These lines can be used for debugging purpose\n",
        "# Or can be seen as a way to build the models\n",
        "\n",
        "# initial_state = encoder.init_states(1)\n",
        "# encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
        "# decoder_outputs = decoder(tf.constant(\n",
        "#     [[1]]), encoder_outputs[1:], encoder_outputs[0])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1JbG-dL1PMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_func(targets, logits):\n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSyITCLEWKNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6S7CF0LEMio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    en_outputs = encoder(source_seq, en_initial_states)\n",
        "    en_states = en_outputs[1:]\n",
        "    de_state_h, de_state_c = en_states\n",
        "    \n",
        "    # We need to create a loop to iterate through the target sequences\n",
        "    for i in range(target_seq_out.shape[1]):\n",
        "      # Input to the decoder must have shape of (batch_size, length)\n",
        "      # so we need to expand one dimension\n",
        "      decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "      logit, de_state_h, de_state_c, _ = decoder(\n",
        "      decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "\n",
        "      # The loss is now accumulated through the whole batch\n",
        "      loss += loss_func(target_seq_out[:, i], logit)\n",
        "      accuracy.update_state(target_seq_out[:, i], logit)\n",
        "      # acc += acc_func(target_seq_out[:, i], logit)\n",
        "  # print(\"acc\",)\n",
        "  # print(acc / target_seq_out.shape[1])\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return loss / target_seq_out.shape[1], accuracy.result()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6AtT0_qPdoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if not os.path.exists('checkpoints_luong/encoder'):\n",
        "  os.makedirs('checkpoints_luong/encoder')\n",
        "if not os.path.exists('checkpoints_luong/decoder'):\n",
        "  os.makedirs('checkpoints_luong/decoder')\n",
        "\n",
        "\n",
        "# Uncomment these lines for inference mode\n",
        "encoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/encoder')\n",
        "decoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/decoder')\n",
        "\n",
        "if encoder_checkpoint is not None and decoder_checkpoint is not None:\n",
        "  encoder.load_weights(encoder_checkpoint)\n",
        "  decoder.load_weights(decoder_checkpoint)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STxOxYjw8u0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afc1a77d-19a1-491a-b9ef-ba5e7cae7207"
      },
      "source": [
        "# train_dataset, test_dataset = train_test_split(dataset,test_size=0.2)\n",
        "# dataset.take(-1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((64, 52), (64, 35), (64, 35)), types: (tf.int32, tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgQbIxk5dWp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf ./checkpoints_luong/encoder\n",
        "! rm -rf ./checkpoints_luong/decoder\n",
        "! mkdir ./checkpoints_luong/encoder\n",
        "! mkdir ./checkpoints_luong/decoder"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3m9U_H_544G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9a218df8-5194-45b3-9930-234709ec9d9a"
      },
      "source": [
        "if MODE == 'train':\n",
        "  for e in range(NUM_EPOCHS):\n",
        "    en_initial_states = encoder.init_states(BATCH_SIZE)\n",
        "    encoder.save_weights('checkpoints_luong/encoder/encoder_{}.h5'.format(e + 1))\n",
        "    decoder.save_weights('checkpoints_luong/decoder/decoder_{}.h5'.format(e + 1))\n",
        "    \n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "      loss, acc = train_step(source_seq, target_seq_in,target_seq_out, en_initial_states)\n",
        "      \n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {}, Batch {}, Loss {:.4f} , Acc {:.4f}'.format(e + 1, batch, loss.numpy(), acc.numpy()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 0, Loss 1.9721 , Acc 0.0000\n",
            "Epoch 2, Batch 0, Loss 0.9815 , Acc 0.0568\n",
            "Epoch 3, Batch 0, Loss 0.8765 , Acc 0.0697\n",
            "Epoch 4, Batch 0, Loss 0.7569 , Acc 0.0765\n",
            "Epoch 5, Batch 0, Loss 0.8037 , Acc 0.0810\n",
            "Epoch 6, Batch 0, Loss 0.8641 , Acc 0.0844\n",
            "Epoch 7, Batch 0, Loss 0.6659 , Acc 0.0872\n",
            "Epoch 8, Batch 0, Loss 0.6744 , Acc 0.0896\n",
            "Epoch 9, Batch 0, Loss 0.5517 , Acc 0.0917\n",
            "Epoch 10, Batch 0, Loss 0.5254 , Acc 0.0937\n",
            "Epoch 11, Batch 0, Loss 0.5420 , Acc 0.0957\n",
            "Epoch 12, Batch 0, Loss 0.4876 , Acc 0.0976\n",
            "Epoch 13, Batch 0, Loss 0.4798 , Acc 0.0997\n",
            "Epoch 14, Batch 0, Loss 0.4232 , Acc 0.1017\n",
            "Epoch 15, Batch 0, Loss 0.4046 , Acc 0.1037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYQ2PCZBnsmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a0b6e430-63f5-471d-f575-daed69ebba87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdegF4MGlpRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r checkpoints_luong \"/content/drive/My Drive/nn/\""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV7vbW8RDvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "  if test_source_text is None:\n",
        "    test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "  test_source_seq = src_tokenizer.texts_to_sequences([test_source_text])\n",
        "  # print(test_source_seq)\n",
        "  \n",
        "  en_initial_states = encoder.init_states(1)\n",
        "  en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
        "  \n",
        "  de_input = tf.constant([[trg_tokenizer.word_index['<start>']]])\n",
        "  de_state_h, de_state_c = en_outputs[1:]\n",
        "  out_words = []\n",
        "  alignments = []\n",
        "  \n",
        "  while True:\n",
        "    de_output, de_state_h, de_state_c, alignment = decoder(de_input, (de_state_h, de_state_c), en_outputs[0])\n",
        "    de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "    out_words.append(trg_tokenizer.index_word[de_input.numpy()[0][0]])\n",
        "    alignments.append(alignment.numpy())\n",
        "    \n",
        "    if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "      break\n",
        "  return np.array(alignments), test_source_text.split(' '), out_words"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu2G4h9M6QQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_src_sents = source[-5:]\n",
        "test_trg_sents = target[-5:]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvac97e0oZKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8516eda4-f07b-44f6-c956-ce88340e8c72"
      },
      "source": [
        "len(test_src_sents)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdICR7A9Gt9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b632bc8c-2eaf-4b9a-cf62-e5aed185af04"
      },
      "source": [
        "! rm -rf heatmap/\n",
        "! mkdir heatmap\n",
        "filenames = []\n",
        "for i, test_sent in enumerate(test_src_sents):\n",
        "  \n",
        "  test_sequence = normalize_string(test_sent)\n",
        "  alignments, source, prediction = predict(test_sequence)\n",
        "  print(\"input:\",test_sent)\n",
        "  print(\"actual:\",test_trg_sents[i])\n",
        "  print(\"predicted:\",' '.join(prediction)[:-6])\n",
        "  print(\"---------\")\n",
        "  attention = np.squeeze(alignments, (1, 2))\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.show()\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='jet')\n",
        "  ax.set_xticklabels([''] + source, rotation=90)\n",
        "  ax.set_yticklabels([''] + prediction)\n",
        "  filenames.append('heatmap/test_{}.png'.format(i))\n",
        "  plt.savefig('heatmap/test_{}.png'.format(i))\n",
        "  plt.close()\n",
        "\n",
        "with imageio.get_writer('translation_heatmaps.gif', mode='I', duration=2) as writer:\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: A group of four kids stand in front of a statue of a large animal .\n",
            "actual: four kids standing\n",
            "predicted: a group of people are playing soccer .\n",
            "---------\n",
            "input: a kid doing tricks on a skateboard on a bridge\n",
            "actual: a kid is skateboarding\n",
            "predicted: a dog is outside .\n",
            "---------\n",
            "input: A dog with a blue collar plays ball outside .\n",
            "actual: a dog is outside\n",
            "predicted: a dog is in the water .\n",
            "---------\n",
            "input: Four dirty and barefooted children .\n",
            "actual: four children have dirty feet .\n",
            "predicted: children are sitting\n",
            "---------\n",
            "input: A man is surfing in a bodysuit in beautiful blue water .\n",
            "actual: On the beautiful blue water there is a man in a bodysuit surfing .\n",
            "predicted: a man is wearing a shirt .\n",
            "---------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rz9ZM1NLyGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! zip -r checkpoints_luong.zip checkpoints_luong\n",
        "# ! zip -r heatmap.zip heatmap\n",
        "! cp -r heatmap \"/content/drive/My Drive/nn/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVpUzbvbeinf",
        "colab_type": "text"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-wOKROgeq1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q nltk"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMLUNLAboj6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/premises_train.pickle?raw=true')\n",
        "outp = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/hypotheses_train.pickle?raw=true')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lgpqu7WhB9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = inp[-1000:]\n",
        "y_test = outp[-1000:]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z8X8nskoUn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80401552-165e-4c14-f1f3-4ee983808eae"
      },
      "source": [
        "len(X_test),len(y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRwuMWOvhL6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_references = []\n",
        "list_of_hypotheses = []\n",
        "\n",
        "for i, test_sent in enumerate(X_test):\n",
        "  \n",
        "  test_sequence = normalize_string(test_sent)\n",
        "  alignments, source, prediction = predict(test_sequence)\n",
        "  list_of_references.append([y_test[i].split()[:-1]])\n",
        "  list_of_hypotheses.append(prediction[:-1])\n",
        "  # print(\"input:\",test_sent)\n",
        "  # print(\"actual:\",test_trg_sents[i])\n",
        "  # print(\"predicted:\",' '.join(prediction)[:-6])\n",
        "  # print(\"---------\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84r_QoTkpYUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd2ad5d0-d222-46dd-8339-5967ac67df66"
      },
      "source": [
        "list_of_references[0]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'group', 'of', 'four', 'are', 'sitting', 'on', 'the', 'sidewalk']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps0MwsvKpcsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ef108f4-3983-4ad3-d0b5-860ccb48ade6"
      },
      "source": [
        "list_of_hypotheses[0]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['people', 'are', 'on', 'a', 'sidewalk']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbKGUOL9eulF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75a7748b-5672-40a9-fa11-06903763d711"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.035916743120916805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}