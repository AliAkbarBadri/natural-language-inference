{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of tempo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakbarbadri/natural-language-inference/blob/master/snli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import imageio"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSNRMGfComKc",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7RsBuPdxU83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODE = 'train'\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_SIZE = 256\n",
        "RNN_SIZE = 512\n",
        "NUM_EPOCHS = 15\n",
        "ATTENTION_FUNC = 'concat'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWeh9G4iRlCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/premises_train.pickle?raw=true')\n",
        "target = pd.read_pickle(r'https://github.com/aliakbarbadri/natural-language-inference/blob/master/hypotheses_train.pickle?raw=true')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJWwX25STJVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf1a6a09-38b0-49a1-f86c-9e6bcd93843d"
      },
      "source": [
        "print(source[0])\n",
        "print(target[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A person on a horse jumps over a broken down airplane .\n",
            "A person is outdoors , on a horse .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCLtsoYAx8b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = source[:5000]\n",
        "trg = target[:5000]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBXfaIKjwAmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCZns_O0xiXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data_src = [normalize_string(data) for data in src]\n",
        "raw_data_trg_in = ['<start> ' + normalize_string(data) for data in trg]\n",
        "raw_data_trg_out = [normalize_string(data) + ' <end>' for data in trg]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amXfHj7vs4uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "# tokenizer.fit_on_texts(raw_data_src)\n",
        "\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "# tokenizer.fit_on_texts(raw_data_trg_in)\n",
        "# tokenizer.fit_on_texts(raw_data_trg_out)\n",
        "\n",
        "# data_src = tokenizer.texts_to_sequences(raw_data_src)\n",
        "# data_src = tf.keras.preprocessing.sequence.pad_sequences(data_src,\n",
        "#                                                         padding='post')\n",
        "# print(data_src[:2])\n",
        "\n",
        "# data_trg_in = tokenizer.texts_to_sequences(raw_data_trg_in)\n",
        "# data_trg_in = tf.keras.preprocessing.sequence.pad_sequences(data_trg_in,\n",
        "#                                                            padding='post')\n",
        "# print(data_trg_in[:2])\n",
        "\n",
        "# data_trg_out = tokenizer.texts_to_sequences(raw_data_trg_out)\n",
        "# data_trg_out = tf.keras.preprocessing.sequence.pad_sequences(data_trg_out,\n",
        "#                                                             padding='post')\n",
        "# print(data_trg_out[:2])\n",
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#     (data_src, data_trg_in, data_trg_out))\n",
        "# dataset = dataset.shuffle(len(raw_data_src)).batch(\n",
        "#     BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMyMHDln3Lkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c826b5ec-da41-499f-a688-6e6baf3f7fd0"
      },
      "source": [
        "src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "src_tokenizer.fit_on_texts(raw_data_src)\n",
        "data_src = src_tokenizer.texts_to_sequences(raw_data_src)\n",
        "data_src = tf.keras.preprocessing.sequence.pad_sequences(data_src,\n",
        "                                                        padding='post')\n",
        "print(data_src[:2])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   1   52    6    1  229  209   70    1 1316   40  545    2    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]\n",
            " [  57  134    5  825   15   66    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1No4Dlqy1T--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dee91c66-d975-4e14-a504-6d5c3cfc4a01"
      },
      "source": [
        "trg_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "trg_tokenizer.fit_on_texts(raw_data_trg_in)\n",
        "trg_tokenizer.fit_on_texts(raw_data_trg_out)\n",
        "data_trg_in = trg_tokenizer.texts_to_sequences(raw_data_trg_in)\n",
        "data_trg_in = tf.keras.preprocessing.sequence.pad_sequences(data_trg_in,\n",
        "                                                           padding='post')\n",
        "print(data_trg_in[:2])\n",
        "\n",
        "data_trg_out = trg_tokenizer.texts_to_sequences(raw_data_trg_out)\n",
        "data_trg_out = tf.keras.preprocessing.sequence.pad_sequences(data_trg_out,\n",
        "                                                            padding='post')\n",
        "print(data_trg_out[:2])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  3   1  19   6  39  11   1 148   2   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  3  15   7  49 378   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "[[  1  19   6  39  11   1 148   2   4   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 15   7  49 378   4   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O20z0Pk3Sdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_src, data_trg_in, data_trg_out))\n",
        "dataset = dataset.shuffle(len(raw_data_src)).batch(\n",
        "    BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRLNhkglwtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, sequence, states):\n",
        "        embed = self.embedding(sequence)\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        return (tf.zeros([batch_size, self.lstm_size]),\n",
        "                tf.zeros([batch_size, self.lstm_size]))\n",
        "\n",
        "\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "\n",
        "encoder = Encoder(src_vocab_size, EMBEDDING_SIZE, RNN_SIZE)\n",
        "initial_state = encoder.init_states(1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orP-CXN233Fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ad797a7-9a70-4e0f-fe8a-e52eec28747f"
      },
      "source": [
        "test_encoder_output = encoder(tf.constant(\n",
        "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
        "print(test_encoder_output[0].shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 6, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amFMhvO0-fZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "  def __init__(self, rnn_size, attention_func):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.attention_func = attention_func\n",
        "    if attention_func not in ['dot', 'general', 'concat']:\n",
        "      raise ValueError(\n",
        "        'Unknown attention score function! Must be either dot, general or concat.')\n",
        "    if attention_func == 'general':\n",
        "    # General score function\n",
        "      self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "    elif attention_func == 'concat':\n",
        "      # Concat score function\n",
        "      self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "      self.va = tf.keras.layers.Dense(1)\n",
        "  def call(self, decoder_output, encoder_output):\n",
        "    if self.attention_func == 'dot':\n",
        "      # Dot score function: decoder_output (dot) encoder_output\n",
        "      # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "      # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "      # => score has shape: (batch_size, 1, max_len)\n",
        "      score = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
        "    elif self.attention_func == 'general':\n",
        "      # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
        "      # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "      # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "      # => score has shape: (batch_size, 1, max_len)\n",
        "      score = tf.matmul(decoder_output, self.wa(encoder_output), transpose_b=True)\n",
        "    elif self.attention_func == 'concat':\n",
        "      # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
        "      # Decoder output must be broadcasted to encoder output's shape first\n",
        "      decoder_output = tf.tile(\n",
        "      decoder_output, [1, encoder_output.shape[1], 1])\n",
        "      # Concat => Wa => va\n",
        "      # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
        "      score = self.va(\n",
        "    self.wa(tf.concat((decoder_output, encoder_output), axis=-1)))\n",
        "    # Transpose score vector to have the same shape as other two above\n",
        "    # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
        "    score = tf.transpose(score, [0, 2, 1])\n",
        "    # alignment a_t = softmax(score)\n",
        "    alignment = tf.nn.softmax(score, axis=2)\n",
        "    # context vector c_t is the weighted average sum of encoder output\n",
        "    context = tf.matmul(alignment, encoder_output)\n",
        "    return context, alignment"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkbglu6bwqwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, rnn_size, attention_func):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.attention = LuongAttention(rnn_size, attention_func)\n",
        "    self.rnn_size = rnn_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(\n",
        "    rnn_size, return_sequences=True, return_state=True)\n",
        "    self.wc = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "    self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self, sequence, state, encoder_output):\n",
        "    # Remember that the input to the decoder\n",
        "    # is now a batch of one-word sequences,\n",
        "    # which means that its shape is (batch_size, 1)\n",
        "    embed = self.embedding(sequence)\n",
        "    # Therefore, the lstm_out has shape (batch_size, 1, rnn_size)\n",
        "    lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
        "    # Use self.attention to compute the context and alignment vectors\n",
        "    # context vector's shape: (batch_size, 1, rnn_size)\n",
        "    # alignment vector's shape: (batch_size, 1, source_length)\n",
        "    context, alignment = self.attention(lstm_out, encoder_output)\n",
        "    # Combine the context vector and the LSTM output\n",
        "    # Before combined, both have shape of (batch_size, 1, rnn_size),\n",
        "    # so let's squeeze the axis 1 first\n",
        "    # After combined, it will have shape of (batch_size, 2 * rnn_size)\n",
        "    lstm_out = tf.concat(\n",
        "                [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "    # lstm_out now has shape (batch_size, rnn_size)\n",
        "    lstm_out = self.wc(lstm_out)\n",
        "    # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
        "    logits = self.ws(lstm_out)\n",
        "    return logits, state_h, state_c, alignment\n",
        "\n",
        "\n",
        "# trg_vocab_size = len(trg_tokenizer.word_index) + 1\n",
        "# decoder = Decoder(trg_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
        "# de_initial_state = test_encoder_output[1:]\n",
        "\n",
        "trg_vocab_size = len(trg_tokenizer.word_index) + 1\n",
        "decoder = Decoder(trg_vocab_size, EMBEDDING_SIZE, RNN_SIZE, ATTENTION_FUNC)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xovyWE9i4NJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These lines can be used for debugging purpose\n",
        "# Or can be seen as a way to build the models\n",
        "\n",
        "# initial_state = encoder.init_states(1)\n",
        "# encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
        "# decoder_outputs = decoder(tf.constant(\n",
        "#     [[1]]), encoder_outputs[1:], encoder_outputs[0])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1JbG-dL1PMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_func(targets, logits):\n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV7vbW8RDvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "  if test_source_text is None:\n",
        "    test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "  test_source_seq = src_tokenizer.texts_to_sequences([test_source_text])\n",
        "  # print(test_source_seq)\n",
        "  \n",
        "  en_initial_states = encoder.init_states(1)\n",
        "  en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
        "  \n",
        "  de_input = tf.constant([[trg_tokenizer.word_index['<start>']]])\n",
        "  de_state_h, de_state_c = en_outputs[1:]\n",
        "  out_words = []\n",
        "  alignments = []\n",
        "  \n",
        "  while True:\n",
        "    de_output, de_state_h, de_state_c, alignment = decoder(de_input, (de_state_h, de_state_c), en_outputs[0])\n",
        "    de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "    out_words.append(trg_tokenizer.index_word[de_input.numpy()[0][0]])\n",
        "    alignments.append(alignment.numpy())\n",
        "    \n",
        "    if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "      break\n",
        "  print(' '.join(out_words))\n",
        "  return np.array(alignments), test_source_text.split(' '), out_words"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6S7CF0LEMio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    en_outputs = encoder(source_seq, en_initial_states)\n",
        "    en_states = en_outputs[1:]\n",
        "    de_state_h, de_state_c = en_states\n",
        "    \n",
        "    # We need to create a loop to iterate through the target sequences\n",
        "    for i in range(target_seq_out.shape[1]):\n",
        "      # Input to the decoder must have shape of (batch_size, length)\n",
        "      # so we need to expand one dimension\n",
        "      decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "      logit, de_state_h, de_state_c, _ = decoder(\n",
        "      decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "    \n",
        "      # The loss is now accumulated through the whole batch\n",
        "      loss += loss_func(target_seq_out[:, i], logit)\n",
        "  \n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  \n",
        "  return loss / target_seq_out.shape[1]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6AtT0_qPdoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('checkpoints_luong/encoder'):\n",
        "  os.makedirs('checkpoints_luong/encoder')\n",
        "if not os.path.exists('checkpoints_luong/decoder'):\n",
        "  os.makedirs('checkpoints_luong/decoder')\n",
        "\n",
        "# Uncomment these lines for inference mode\n",
        "encoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/encoder')\n",
        "decoder_checkpoint = tf.train.latest_checkpoint('checkpoints_luong/decoder')\n",
        "\n",
        "if encoder_checkpoint is not None and decoder_checkpoint is not None:\n",
        "  encoder.load_weights(encoder_checkpoint)\n",
        "  decoder.load_weights(decoder_checkpoint)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STxOxYjw8u0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "# accuracy.update_state(target , prediction)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3m9U_H_544G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "08a9eafd-c8e6-4955-ae30-fa1d9846a26f"
      },
      "source": [
        "if MODE == 'train':\n",
        "  for e in range(NUM_EPOCHS):\n",
        "    en_initial_states = encoder.init_states(BATCH_SIZE)\n",
        "    encoder.save_weights('checkpoints_luong/encoder/encoder_{}.h5'.format(e + 1))\n",
        "    decoder.save_weights('checkpoints_luong/decoder/decoder_{}.h5'.format(e + 1))\n",
        "    \n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "      loss = train_step(source_seq, target_seq_in,target_seq_out, en_initial_states)\n",
        "      \n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1, batch, loss.numpy()))\n",
        "    try:\n",
        "      predict()\n",
        "      predict(\"a dog jumping over a stream of water\")\n",
        "    except Exception:\n",
        "      continue"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9682\n",
            "Epoch 2 Batch 0 Loss 1.0649\n",
            "Epoch 3 Batch 0 Loss 0.7715\n",
            "Epoch 4 Batch 0 Loss 0.8348\n",
            "Epoch 5 Batch 0 Loss 0.7355\n",
            "Epoch 6 Batch 0 Loss 0.7869\n",
            "Epoch 7 Batch 0 Loss 0.6318\n",
            "Epoch 8 Batch 0 Loss 0.7205\n",
            "Epoch 9 Batch 0 Loss 0.6085\n",
            "Epoch 10 Batch 0 Loss 0.5412\n",
            "Epoch 11 Batch 0 Loss 0.5114\n",
            "Epoch 12 Batch 0 Loss 0.4583\n",
            "Epoch 13 Batch 0 Loss 0.5077\n",
            "Epoch 14 Batch 0 Loss 0.4196\n",
            "Epoch 15 Batch 0 Loss 0.3943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rz9ZM1NLyGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! zip -r checkpoints_luong.zip checkpoints_luong\n",
        "! zip -r heatmap.zip heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu2G4h9M6QQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sents = source[-5:]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR_7CPoeJ_FZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d6f543f-fb2d-4f00-c9f2-07d29d9b1dd7"
      },
      "source": [
        "test_sents[-5]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A group of four kids stand in front of a statue of a large animal .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdICR7A9Gt9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4b9e4e1b-e37e-4f87-d00e-c58878e71b51"
      },
      "source": [
        "! rm -rf heatmap/\n",
        "! mkdir heatmap\n",
        "filenames = []\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "  print(test_sent)\n",
        "  test_sequence = normalize_string(test_sent)\n",
        "  alignments, source, prediction = predict(test_sequence)\n",
        "  attention = np.squeeze(alignments, (1, 2))\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='jet')\n",
        "  ax.set_xticklabels([''] + source, rotation=90)\n",
        "  ax.set_yticklabels([''] + prediction)\n",
        "  filenames.append('heatmap/test_{}.png'.format(i))\n",
        "  plt.savefig('heatmap/test_{}.png'.format(i))\n",
        "  plt.close()\n",
        "\n",
        "with imageio.get_writer('translation_heatmaps.gif', mode='I', duration=2) as writer:\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A group of four kids stand in front of a statue of a large animal .\n",
            "the group of people are outside . <end>\n",
            "a kid doing tricks on a skateboard on a bridge\n",
            "a small dog is running outside . <end>\n",
            "A dog with a blue collar plays ball outside .\n",
            "a dog is running . <end>\n",
            "Four dirty and barefooted children .\n",
            "kids are playing with a game . <end>\n",
            "A man is surfing in a bodysuit in beautiful blue water .\n",
            "a man is outside . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}